from mathvision_helper_functions import is_equal

import os
import math
import re
import json
import numpy as np
from tqdm import tqdm
from pathlib import Path
from collections import Counter

import argparse

def is_answer_match(a, b):
    a, b = str(a).strip(), str(b).strip()
    if a.lower() == b.lower(): return True
    try: return abs(float(a) - float(b)) < 1e-10
    except: return False

def extract_boxed(s):
    # Match both \boxed{...} and \\boxed{...} patterns to handle escape sequence issues
    # Use raw string in pattern but flexible matching for corrupted backslashes
    patterns = [
        r"\\boxed\{([^{}]*(?:\{[^{}]*\}[^{}]*)*)\}",  # Standard \boxed{...}
        r"\x08oxed\{([^{}]*(?:\{[^{}]*\}[^{}]*)*)\}",  # Handle corrupted \b -> \x08 from LLM string output, as LLM may not know to escape the backslash all the time
        r"\nboxed\{([^{}]*(?:\{[^{}]*\}[^{}]*)*)\}",  # Handle \nboxed (generated by 7B model)
        r"\\\[(.*?)\]",
        r"```(?:\n)?(.*?)\n?```",
        r"Answer:\s*(.+)",
    ]

    all_matches = []
    for pattern in patterns:
        matches = list(re.finditer(pattern, s))
        all_matches.extend(matches)

    if not all_matches:
        return None

    # Sort by position and return the LATEST match
    all_matches.sort(key=lambda m: m.start())
    return all_matches[-1].group(1).strip()

def load_json(file_path):
    """
    Load a JSON file and return its contents as a Python object (dict or list).
    """
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data

def main():
    parser = argparse.ArgumentParser(
        description="Compute mathvision score"
    )
    parser.add_argument(
        "--data-path",
        type=str,
        # default="/home/ubuntu/porialab-us-midwest-1/Tej/mmr-eval/traces_data",
        required = True,
        help="Path to step traces",
    )

    args = parser.parse_args()

    path = args.data_path

    print(f"Data Path: {path}")

    # filename = path.name

    data = load_json(path)

    score = 0
    score_null = 0
    
    first_score = 0
    first_score_null = 0

    major_score = 0

    # is_iter = False

    total = 0

    for sidx, sample in enumerate(tqdm(data)):
        # if "<image2>" not in sample['annotation']['question']:
        #     continue
        gt_answer = sample["gt_answer"]
        pred_answer = sample['pred_answer']

        if pred_answer == None:
            pred_answer = extract_boxed(sample['raw_full_prediction'])

        options = sample['annotation']['options']
        
        if len(options) > 0:
            gt_answer_value = options[ord(gt_answer)-ord('A')]
        else:
            gt_answer_value = ''

        if pred_answer == None:
            score_null += 1
        
        if pred_answer and (is_equal(gt_answer, pred_answer) or is_equal(gt_answer_value, pred_answer)):
            score += 1

        if 'iteration_history' in sample:
            first_candidate = sample['iteration_history'][0]['candidates_info'][0]
            first_answer = extract_boxed(first_candidate["candidate_step"])

            if first_answer == None:
                first_score_null += 1

            candidates = sample['iteration_history'][0]['candidates_info']
            candidate_preds = [extract_boxed(candidate["candidate_step"]) for candidate in candidates]
            
            maj_counter = Counter(candidate_preds)
            majority_string, count = maj_counter.most_common(1)[0]
        
            if first_answer and (is_equal(gt_answer, first_answer) or is_equal(gt_answer_value, first_answer)):
                first_score += 1

            if majority_string and (is_equal(gt_answer, majority_string) or is_equal(gt_answer_value, majority_string)):
                major_score += 1

        total += 1
    
    print(f"PRM Score: {score}/{total} ({score_null} Failed extraction)", score/total)
    print(f"First Score: {first_score}/{total} ({first_score_null} Failed extraction)", first_score/total)
    print(f"Majority Score: {major_score}/{total}", major_score/total)



if __name__ == "__main__":
    main()

# /scratch_aisg/SPEC-SF-AISG/ob1/mmr-eval/evaluation/base_model_eval_code
# /scratch_aisg/SPEC-SF-AISG/ob1/mmr-eval/evaluation/reward_guided_search