#!/bin/bash
#PBS -q AISG_large
#PBS -j oe
#PBS -k oed

#PBS -l select=1:ngpus=8:ncpus=96:mem=1800gb
#PBS -l walltime=336:00:00

# PBS handles output redirection via -o option

# PBS output file handling - adjust path for qwen_training directory
PBS_LOG_PATH="${PBS_O_WORKDIR}/qwen_training/${PBS_OUTPUT_FILE}"
echo "PBS_OUTPUT_FILE: ${PBS_LOG_PATH}"
# Ensure immediate log flushing
exec > >(stdbuf -oL tee -a "${PBS_LOG_PATH}")
exec 2>&1

cd $PBS_O_WORKDIR

# Navigate to qwen_training directory (assuming PBS is submitted from parent directory)
cd qwen_training

# Activate the virtual environment
source .venv/bin/activate
echo "Python path after activation: $(which python)"
echo "Python version: $(python --version)"

# Source environment file if it exists, otherwise warn
if [ -f ".env.pbs" ]; then
    source .env.pbs
else
    echo "WARNING: .env.pbs file not found, continuing without it"
fi

export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export NCCL_DEBUG=INFO
export HF_HOME=/scratch_aisg/SPEC-SF-AISG/cache/huggingface

# Force immediate output flushing
export PYTHONUNBUFFERED=1

# Quick CUDA health check before training (minimal overhead ~5-10 seconds)
echo "Running CUDA health check..."
python -c "
import torch
import sys

try:
    # Quick availability check
    if not torch.cuda.is_available():
        print('CUDA not available')
        sys.exit(1)
    
    num_gpus = torch.cuda.device_count()
    print(f'Found {num_gpus} GPUs')
    
    # Lightweight memory test - just test first and last GPU
    # This catches most faulty node issues without excessive overhead
    test_gpus = [0, num_gpus-1] if num_gpus > 1 else [0]
    
    for gpu_id in test_gpus:
        device = torch.device(f'cuda:{gpu_id}')
        
        # Single memory allocation and operation per GPU
        # Size is large enough to catch memory issues but small enough to be quick
        test_tensor = torch.randn(2048, 2048, device=device, dtype=torch.float16)
        result = test_tensor @ test_tensor.T
        
        # Critical: Force synchronization to catch async errors
        torch.cuda.synchronize(device)
        
        # Memory transfer test (catches PCIe issues)
        cpu_tensor = torch.randn(1024, 1024, dtype=torch.float16)
        gpu_tensor = cpu_tensor.to(device)
        _ = gpu_tensor.cpu()
        torch.cuda.synchronize(device)
        
        print(f'  GPU {gpu_id}: OK')
    
    print('CUDA health check PASSED')
    sys.exit(0)
    
except Exception as e:
    print(f'CUDA health check FAILED: {e}')
    sys.exit(1)
"
cuda_check=$?

if [ $cuda_check -ne 0 ]; then
    echo "ERROR: CUDA health check failed!"
    echo "Node appears faulty. Exiting with code 99 for automatic requeue."
    exit 99
fi

echo "CUDA check passed. Proceeding with training..."

# WandB configuration
export WANDB_ENTITY="aisg-arf"
export WANDB_PROJECT="multimodal-reasoning"

# Distributed training configuration
MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}
MASTER_PORT=${MASTER_PORT:-$(shuf -i 20001-29999 -n 1)}
NNODES=${WORLD_SIZE:-1}
NPROC_PER_NODE=$(nvidia-smi --list-gpus | wc -l)  # Automatically detects available GPUs

# DeepSpeed configuration
deepspeed=./scripts/zero3.json

# Model configuration from environment variable
llm=${MODEL_PATH}

# Dataset configuration from environment variable
datasets=${DATASET_NAME}

# Vision tuning configuration from environment variable (default: False)
tune_mm_vision=${TUNE_MM_VISION:-False}

# Training hyperparameters (matching sft_7b.sh)
lr=2e-7
batch_size=2
grad_accum_steps=8

# Training entry point
entry_file=qwenvl/train/train_qwen.py

# Output configuration with vision tuning suffix
vision_suffix=""
if [ "${tune_mm_vision}" = "True" ]; then
    vision_suffix="_vit_trained"
else
    vision_suffix="_vit_frozen"
fi

run_name="$(echo "${datasets}" | cut -d'%' -f1)-$(basename "${llm}")-$(date +%Y%m%d_%H%M%S)${vision_suffix}"
output_dir="./outputs/${run_name}"

echo "Starting training at $(date)"
echo "Working directory: $PWD"
echo "Python path: $(which python)"
echo "CUDA devices: $CUDA_VISIBLE_DEVICES"
echo "Model: ${llm}"
echo "Dataset: ${datasets}"
echo "Learning rate: ${lr}"
echo "Batch size: ${batch_size}"
echo "Gradient accumulation steps: ${grad_accum_steps}"
echo "Output directory: ${output_dir}"

# Training arguments
args="
    --deepspeed ${deepspeed} \
    --model_name_or_path "${llm}" \
    --dataset_use ${datasets} \
    --data_flatten True \
    --tune_mm_vision ${tune_mm_vision} \
    --tune_mm_mlp True \
    --tune_mm_llm True \
    --bf16 \
    --output_dir ${output_dir} \
    --num_train_epochs 2 \
    --per_device_train_batch_size ${batch_size} \
    --per_device_eval_batch_size $((batch_size*2)) \
    --gradient_accumulation_steps ${grad_accum_steps} \
    --max_pixels 200704 \
    --min_pixels 12544 \
    --eval_strategy "no" \
    --save_strategy "steps" \
    --save_steps 1000 \
    --save_total_limit 1 \
    --learning_rate ${lr} \
    --weight_decay 0 \
    --warmup_ratio 0.03 \
    --max_grad_norm 1 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --gradient_checkpointing True \
    --dataloader_num_workers 4 \
    --run_name ${run_name} \
    --report_to wandb"

# Launch training
torchrun --nproc_per_node=${NPROC_PER_NODE} \
         --master_addr=${MASTER_ADDR} \
         --master_port=${MASTER_PORT} \
         ${entry_file} ${args}

exit_code=$?
echo "Training completed with exit code: $exit_code at $(date)"

exit $exit_code